{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-2 Loss와 Metric\n",
    "\n",
    "- Loss: 학습 모델 시 학습데이터(train data)를 바탕으로 계산되어, 모델의 파라미터 업데이트에 활용되는 함수\n",
    "- Metric: 모델 학습 종료 후 테스트데이터(test data)ㄹㄹ 바탕으로 계산되어, 학습된 모델의 성능을 평가하는데 활용되는 함수\n",
    "\n",
    "MNIST 손글씨 분류 모델 평가\n",
    "- 분류 모델의 Loss함수: Entropy Loss\n",
    "- 분류 모델의 Metric(학습모델평가): Acurracy\n",
    "\n",
    "accuracy가 좋은 학습평가 모델이 될 수 없는 이유?\n",
    "\n",
    "Discrete한 Accuracy는 Continuous한 Cross Entropy에 비해, 파라미터가 학습되어야 할 방향을 정확하게 제시하지 못한다. 비록 위 예시에서 1번 케이스는 Accuracy는 2번 케이스보다 낮았지만, Loss를 낮추는 쪽으로 더 학습이 진행되면 Accuracy도 높이면서 A, B, C간 확률분포의 차이(0.7-0.15)를 더욱 뚜렷하게 해서 결과적으로는 더 명확한 분류 기준을 학습한 모델이 될 여지가 있다.\n",
    "\n",
    "Accuracy를 기준으로 학습이 진행된다면 2번 케이스는 이미 완벽한 Accuracy에 도달했으므로 더이상 정교한 학습이 이루어질 여지가 없어서(Accuracy가 loss가 된다면 loss=0) A, B, C간 확률분포의 차이(0.5-0.25)가 더 커지도록 학습이 진행되지 않을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-3 Confusion Matrix 와 Precision/Recall\n",
    "\n",
    "분류 모델로 되돌아가 보자\n",
    "하지만 우리는 분류모델에서 Accuracy가 항상 좋은 Metric이 되지 않을 수 있다\n",
    "분류 모델 측면에서 모델의 결과가 이진 분류되는 형태일 때, 모델의 예측 결과와 실제 정답셋을 비교하여 아래의 표처럼 4가지 항목으로 표현하는 Confusion Matrix을 떠올려 보자\n",
    "\n",
    "- True Positive(TP) - 모델이 양성(Positive)을 양성으로 맞혔을 때\n",
    "- True Negative(TN) - 모델이 음성(Negative)을 음서으로 맞혔을 때\n",
    "- False Positive(FP) - 모델이 음성(Negative)을 양성(Positive)으로 잘못 예측했을 때\n",
    "- False Negative(FN) - 모델이 양성(Positive)을 음성(Negative)으로 잘못 예측했을 때\n",
    "\n",
    "Accuracy 는 전체 표본 중 정확히 분류된 표본의 수   \n",
    "\n",
    "병원에 내원한 환자가 암인지 아닌지 구분하는 예측모델을 만드는 경우를 생각해봅시다. 100명의 환자에 대한 예측에 대해 다음과 같은 결과를 얻었습니다.\n",
    "\n",
    "- TP(실제로 암이면서, 암으로 예측한 결과) = 1\n",
    "- TN(실제로 정상이고, 정상으로 예측한 결과) = 90\n",
    "- FN(실제로 암이지만, 정상으로 예측한 결과) = 8\n",
    "- FP(실제로 정상이지만, 암으로 예측한 결과) = 1\n",
    "\n",
    "정확도: 0.91   \n",
    "   \n",
    "정확도는 이 문제의 평가 척도로 적절한가? 잘못 분류된 9명은 평가 척도에서 어떻게 고려되는게 좋을까?\n",
    "\n",
    "___\n",
    "Precision 과 Recall\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-4 Threshold의 변화에 따른 모델 성능\n",
    "\n",
    "학습시킨 모델의 출력이 일정 이상 기준선(Threshold)를 넘어가면 우리는 양성이라고 분류한다. 우리가 학습시킨 모델은 Recall이 높을 수록 좋은 모델\n",
    "\n",
    "- 양성확률이 0.5가 넘으면 양성이라고 분류하는 것보다 양성일 확률이 0.3만 넘으면 양성이라고 분류하도록 해보면 어떨까?\n",
    "\n",
    "- 모델의 파라미터등은 전혀 변한 것이 없는데 모델의 출력값을 해석하는 방식만 다르게 해도 모델의 성능은 변한다\n",
    "\n",
    "- 모델의 성능이라는 것은 F1 score같은 숫자 하나로만 규정될 수 있는게 아니다.\n",
    "\n",
    "- Threshold가 달라지는 것을 고려해 전체적인 모델의 성능을 평가하는 방법으로 PR(Precision and Recall) 커브와 ROC(Reciver Operating Characteristic) 커브를 그리는 두 가지 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape) # 4개의 feature를 가진 150개의 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task의 재구성\n",
    "___\n",
    "붗꽃 데이터에 잡음 데이터를 넣고 성능을 일부러 낮춘다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 804)\n"
     ]
    }
   ],
   "source": [
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "print(X.shape)  # 804개의 feature를 가진 150개의 데이터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련, 테스트 셋에 사용된 라벨의 종류: {0, 1} \n",
      "훈련 데이터 shape   : (50, 804)\n",
      "테스트 데이터 shape : (50, 804)\n"
     ]
    }
   ],
   "source": [
    "#- 0, 1 라벨에 속하는 붓꽃 샘플만 사용하도록 제한합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "print(\"훈련, 테스트 셋에 사용된 라벨의 종류: {} \".format( set(y_test)))\n",
    "print(\"훈련 데이터 shape   :\", X_train.shape)\n",
    "print(\"테스트 데이터 shape :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습\n",
    "___\n",
    "2개의 랍ㄹ로 나뉜 데이터를 갖고, SVM(Support Vector Machine) 으로 모델을 구성하여 학습시킨후 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel='poly', random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel='linear', random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29512751  0.28798352  0.17635465  0.19056886  0.38391605 -0.30841065\n",
      " -0.10084254 -0.23481309  0.18576987 -0.36011033 -0.15726747 -0.25714889\n",
      " -0.14979669  0.02063898  0.04509171 -0.17239443  0.07287957 -0.0689103\n",
      " -0.13452462 -0.30697712  0.25404241 -0.28916471 -0.52061453  0.25252233\n",
      "  0.02177777 -0.10980907  0.37468422  0.35303004 -0.6211302  -0.42920064\n",
      " -0.14770647  0.00593404 -0.34735296  0.32245409 -0.19439024  0.1288847\n",
      " -0.0320947  -0.23008604 -0.10135548 -0.46962186  0.05184235  0.0609688\n",
      "  0.05632596  0.44769206 -0.38804349  0.24704844  0.16063684  0.0144203\n",
      " -0.03136574  0.11179177]\n"
     ]
    }
   ],
   "source": [
    "y_score = classifier.decision_function(X_test)\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  4]\n",
      " [ 6 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80        24\n",
      "           1       0.83      0.77      0.80        26\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.80      0.80      0.80        50\n",
      "weighted avg       0.80      0.80      0.80        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)\n",
    "rpt_result = classification_report(y_test, y_pred)\n",
    "print(rpt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  6]\n",
      " [ 5 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.77        24\n",
      "           1       0.78      0.81      0.79        26\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.78      0.78      0.78        50\n",
      "weighted avg       0.78      0.78      0.78        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_new_threshold = classifier.decision_function(X_test) > -0.1\n",
    "conf_mat = confusion_matrix(y_test, y_pred_new_threshold)\n",
    "print(conf_mat)\n",
    "rpt_result = classification_report(y_test, y_pred_new_threshold)\n",
    "print(rpt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 12]\n",
      " [ 2 24]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_new_threshold = classifier.decision_function(X_test)>-0.2\n",
    "conf_mat = confusion_matrix(y_test,y_pred_new_threshold)\n",
    "print(conf_mat)\n",
    "rpt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
