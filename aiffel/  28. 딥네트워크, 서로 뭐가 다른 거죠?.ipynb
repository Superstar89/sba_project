{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-1. 딥네트워크, 서로 뭐가 다른 거죠?\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## 28-2 ImageNet Challenge\n",
    "\n",
    "- 이미지넷은 비전(Vision) 관련 딥러닝을 하면 마주치게 되는 이름\n",
    "\n",
    "### 이미지 분류 모델 평가에 사용되는 TOP-5 error와 TOP-1 error\n",
    "\n",
    "- top5-error 와 top-1 error 라는 용어는 이미지 분류(image classification)성을을 평가하기 위한 것이다.\n",
    "- 훈련된 분류기는 주어진 이미지가 어떤 클래스에 속하는지 분류해 낼 것 이다.\n",
    "- {고양이, 강아지, 집, 축구공, 사자}와 같이 5개의 클래스 라벨을 갖고 훈련된 분류기가 있다고 가정하자\n",
    "- {0.1, 0.6, 0.05, 0.05, 0.2} 과 같은 결과를 냈을때 새로운 이미지는 강아지일 확률이 가장 높다고 판단\n",
    "- 이때 top-1 class는 {강아지}가 된다. 그리고 top-2 class는 {강아지, 사자}가 된다.\n",
    "- 분류기가 새로운 이미지들에 대해 예측한 클래스인 top-1 class가 실제 클래스와 같다면 이때 top-1 error는 0%가 된다\n",
    "- 또한 분류기가 높은 확률로 예츠한 상위 5개의 클래스, 즉 top5-class중에 실제 클래스가 있다면 이때 top-5 error는 무조건 0%가 된다.\n",
    "\n",
    "- 1000개의 클래스로 훈련된 분류기의  top-5 error가 5% 보다 작으면, 분류 능력이 상당히 좋다고 판단 할 수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-3 딥네트워크의 시작\n",
    "\n",
    "- Supervision 팀이 전년도 대비 10%의 오류율을 낮추면서 1등을 할 수 있었던 비법은 네트워크를 깊게 쌓았기 때문입니다. \n",
    "\n",
    "### AlexNet 구조\n",
    "AlexNet의 기본구조는 LeNet-5와 크게 다르지 않다. 2개의 GPU로 병렬연산을 수행하기 위해 병렬적인 구조로 설계되었다는 점이 가장 큰 변화다\n",
    "\n",
    "- AlexNet은 8개의 레이어로 구성되어 있다. 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어 있다. \n",
    "\n",
    "- 두 번째, 네번째, 다섯번째 컨볼루션 레이어는 전 단계의 두 채널의 특성맵들과 모두 연결되어 있다는 것을 집고 넘어가자\n",
    "\n",
    "1) __첫번쨰 레이어(컨볼루션 레이어)__ : \n",
    "- 96개의 11 X 11 X 3 사이즈 필터커널로 입력 영상을 컨볼루션한다\n",
    "- 컨볼루션 보폭(stride)를 4로 설정, zero-padding은 사용하지 않았다.\n",
    "- zero-padding은 컨볼루션으로 인해 특성맵의 사이즈가 축소되는 것을 방지 또는 축소되는 것을 줄이기 위해 영상의 가장자리 부분에 0을 추가한ㄴ ㅡ것이다 \n",
    "- 결과적으로 55 X 55 x 96 특성맵(96장의 55x55 사이즈 특성맵들)이 산출된다.\n",
    "- 그 다음 ReLU 함수로 활성화 해준다\n",
    "- 이어서 3 x 3 overlapping max pooling이 stride2 로 실행이된다. 그 결과 27 x 27 x 96 특성 맵을 갖게 된다.\n",
    "\n",
    "- 그 다음에는 수렴 속도를 높이기 위해 local response normalization이 시행\n",
    "- local response normalization은 특성맵의 차원을 변화시ㅣ지 않으므로, 특성맵의 크기는 27x27x96으로 유지된다.\n",
    "\n",
    "2) __두번째 레이어(컨볼루션 레이어)__:\n",
    "- 256개의 5 x 5 x 48 커널을 사용하여 전 단계의 특성맵을 컨볼루션해준다. \n",
    "- stride는 1로, zero-padding은 2로 설정했다. \n",
    "- 따라서 27 x 27 x 256 특성맵(256장의 27 x 27 사이즈 특성맵들)을 얻게 된다. \n",
    "- ReLU 함수로 활성화한다. \n",
    "- 그 다음에 3 x 3 overlapping max pooling을 stride 2로 시행한다. \n",
    "- 그 결과 13 x 13 x 256 특성맵을 얻게 된다. \n",
    "- 그 후 local response normalization이 시행되고, 특성맵의 크기는 13 x 13 x 256으로 그대로 유지된다. \n",
    "\n",
    "3) __세번째 레이어(컨볼루션 레이어)__:\n",
    "- 384개의 3 x 3 x 256 커널을 사용하여 전 단계의 특성맵을 컨볼루션해준다. \n",
    "- stride와 zero-padding 모두 1로 설정한다. \n",
    "- 따라서 13 x 13 x 384 특성맵(384장의 13 x 13 사이즈 특성맵들)을 얻게 된다. \n",
    "- ReLU 함수로 활성화한다. \n",
    "\n",
    "4) __네번째 레이어(컨볼루션 레이어)__:\n",
    "- 384개의 3 x 3 x 192 커널을 사용해서 전 단계의 특성맵을 컨볼루션해준다.\n",
    "- stride와 zero-padding 모두 1로 설정한다.\n",
    "- 따라서 13 x 13 x 384 특성맵(384장의 13 x 13 사이즈 특성맵들)을 얻게 된다.\n",
    "- ReLU 함수로 활성화한다. \n",
    "\n",
    "5) __다섯번째 레이어(컨볼루션 레이어):\n",
    "- 256개의 3 x 3 x 192 커널을 사용해서 전 단계의 특성맵을 컨볼루션해준다. \n",
    "- stride와 zero-padding 모두 1로 설정한다. \n",
    "- 따라서 13 x 13 x 256 특성맵(256장의 13 x 13 사이즈 특성맵들)을 얻게 된다. \n",
    "- 역시 ReLU 함수로 활성화한다.\n",
    "- 그 다음에 3 x 3 overlapping max pooling을 stride 2로 시행한다.\n",
    "- 그 결과 6 x 6 x 256 특성맵을 얻게 된다.\n",
    "\n",
    "6) __여섯번째 레이어(Fully connected layer):__\n",
    "- 6 x 6 x 256 특성맵을 flatten해줘서 6 x 6 x 256 = 9216차원의 벡터로 만들어준다. \n",
    "- 그것을 여섯번째 레이어의 4096개의 뉴런과 fully connected 해준다. \n",
    "- 그 결과를 ReLU 함수로 활성화한다. \n",
    "\n",
    "7) __일곱번째 레이어(Fully connected layer):__\n",
    "- 4096개의 뉴런으로 구성되어 있다. \n",
    "- 전 단계의 4096개 뉴런과 fully connected되어 있다. \n",
    "- 출력 값은 ReLU 함수로 활성화된다.\n",
    "\n",
    "8) __여덟번째 레이어(Fully connected layer):__\n",
    "- 1000개의 뉴런으로 구성되어 있다. \n",
    "- 전 단계의 4096개 뉴런과 fully connected되어 있다. \n",
    "- 1000개 뉴런의 출력값에 softmax 함수를 적용해 1000개 클래스 각각에 속할 확률을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-4 CNN을 잘 쓰자\n",
    "\n",
    "### CNN 알고리즘들 - VGGNet의 구조(VGG16)\n",
    "\n",
    "#### VGGNet의 구조\n",
    "\n",
    "- 이 연구의 핵심은 네트워크의 깊이를 깊게 만드는 것이 성능에 어떤 영향을 미치는지 확인하고자 한 것\n",
    "- 필터 커널의 사이즈가 크면 이미지의 사이즈가 금방 축소되기 때문에 네트워크의 깊이를 충분히 깊게 만들기 어렵다\n",
    "\n",
    "- VGG 연구팀은 AlexNet과 VGG-F, VGG-M, VGG-S에서 사용되던 Local Response Normalization(LRN)이 A 구조와 A-LRN 구조의 성능을 비교함으로 성능 향상에 별로 효과가 없다고 실험을 통해 확인\n",
    "\n",
    "- 깊이가 11층, 13층, 16층, 19층으로 깊어지면서 분류 에러가 감소하는 것을 관찰했다. 즉, 깊어질수록 성능이 좋아진다는 것이었다.\n",
    "\n",
    "#####  3 x 3 필터로 하는 것이 7 x 7 필터로 한 번 컨볼루션 하는 것 보다 나은 점?\n",
    "- 일단 가중치 또는 파라미터의 갯수의 차이다\n",
    "- 3 x 3 필터가 3개면 총 27개의 가중치를 갖는다. 반면 7 x 7 필터는 49개의 가중치를 갖는다.\n",
    "- CNN에서 가중치는 모두 훈련이 필요한 것이므로 가중치가 적을 수록 그만큼 훈련시켜야 할 것의 갯수는 작아진다\n",
    "- 따라서 학습의 속도가 빠르다\n",
    "- 동시에 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가시키기 때문에 특성이 점점 더 유용해진다\n",
    "\n",
    "#### VGG16 구조 분석\n",
    "\n",
    "\n",
    "### CNN(GoogLeNet)\n",
    "- 신경망이 깊어지면 2 가지 중대한 문제 발생\n",
    "\n",
    "- 망이 커질수록 자유파라미터의 수가 증가하게 되며, overfitting에 빠질 가능성이 높다\n",
    "- 대량의 데이터에서 일일이 label 을 달기 어렵다\n",
    "- 망의 크기가 커질수록 연산망이 느러난다\n",
    "- 필터의 갯수가 증가하게 되면 연산량은 제곱으로 늘어나게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-5 멀리 있으면 잘 안 들려요\n",
    "\n",
    "### Vanishing gradient\n",
    "ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-6. 지름길을 만들어주자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-8 Model API\n",
    "\n",
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-9 VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 35s 0us/step\n",
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.applications import imagenet_utils\n",
    "\n",
    "# CLFAR100 데이터셋을 가져온다\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test)= cifar100.load_data()\n",
    "x_train, x_train = x_train/255.0, x_test/255.0\n",
    "\n",
    "print(len(x_train),len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 325,956\n",
      "Trainable params: 325,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = keras.Input(shape=(32,32,3))\n",
    "\n",
    "x = keras.layers.Conv2D(16,3, activation='relu')(img_input)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Conv2D(32,3, activation='relu')(x)\n",
    "x = keras.layers.MaxPool2D((2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(100, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=img_input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 10000\n  y sizes: 50000\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7c0f760499ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 1 Epoch만 학습합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 10000\n  y sizes: 50000\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "# 모델 학습!! \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)    # 1 Epoch만 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet50 GoGo!\n"
     ]
    }
   ],
   "source": [
    "# 추가로 import 해야 할 패키지들을 먼저 가져옵니다.\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "# block 안에 반복적으로 활용되는 L2 regularizer를 선언해 줍니다.\n",
    "def _gen_l2_regulariztion(use_l2_regularizer= True, l2_weight_decay=1e-4):\n",
    "    return regularizers.l2(l2_weight_decay) if use_l2_regularizer else None\n",
    "\n",
    "print('Resnet50 GoGo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2),\n",
    "               use_l2_regularizer=True,\n",
    "               batch_norm_decay=0.9,\n",
    "               batch_norm_epsilon=1e-5):\n",
    "  \"\"\"A block that has a conv layer at shortcut.\n",
    "  Note that from stage 3,\n",
    "  the second conv layer at main path is with strides=(2, 2)\n",
    "  And the shortcut should have strides=(2, 2) as well\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "    filters: list of integers, the filters of 3 conv layer at main path\n",
    "    stage: integer, current stage label, used for generating layer names\n",
    "    block: 'a','b'..., current block label, used for generating layer names\n",
    "    strides: Strides for the second conv layer in the block.\n",
    "    use_l2_regularizer: whether to use L2 regularizer on Conv layer.\n",
    "    batch_norm_decay: Moment of batch norm layers.\n",
    "    batch_norm_epsilon: Epsilon of batch borm layers.\n",
    "  Returns:\n",
    "    Output tensor for the block.\n",
    "  \"\"\"\n",
    "  filters1, filters2, filters3 = filters\n",
    "  if backend.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "  else:\n",
    "    bn_axis = 1\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters1, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2a')(\n",
    "          input_tensor)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=batch_norm_decay,\n",
    "      epsilon=batch_norm_epsilon,\n",
    "      name=bn_name_base + '2a')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters2,\n",
    "      kernel_size,\n",
    "      strides=strides,\n",
    "      padding='same',\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=batch_norm_decay,\n",
    "      epsilon=batch_norm_epsilon,\n",
    "      name=bn_name_base + '2b')(\n",
    "          x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '2c')(\n",
    "          x)\n",
    "  x = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=batch_norm_decay,\n",
    "      epsilon=batch_norm_epsilon,\n",
    "      name=bn_name_base + '2c')(\n",
    "          x)\n",
    "\n",
    "  shortcut = layers.Conv2D(\n",
    "      filters3, (1, 1),\n",
    "      strides=strides,\n",
    "      use_bias=False,\n",
    "      kernel_initializer='he_normal',\n",
    "      kernel_regularizer=_gen_l2_regularizer(use_l2_regularizer),\n",
    "      name=conv_name_base + '1')(\n",
    "          input_tensor)\n",
    "  shortcut = layers.BatchNormalization(\n",
    "      axis=bn_axis,\n",
    "      momentum=batch_norm_decay,\n",
    "      epsilon=batch_norm_epsilon,\n",
    "      name=bn_name_base + '1')(\n",
    "          shortcut)\n",
    "\n",
    "  x = layers.add([x, shortcut])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
