{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 23-2 신경망 구성 (1) 개요\n",
    "\n",
    "신경망이란?   \n",
    "   \n",
    "신경계 뉴런이 그물망 형태로 이루고 있는 것을 신경망이라고 부른다   \n",
    "   \n",
    "### MNIST Revisited\n",
    "___\n",
    "MNIST 이미지 모델 분류기 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jaejin/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.5064 - acc: 0.8795\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2328 - acc: 0.9352\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1826 - acc: 0.9483\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.1524 - acc: 0.9567\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1315 - acc: 0.9620\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.1149 - acc: 0.9675\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1028 - acc: 0.9703\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0926 - acc: 0.9732\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0839 - acc: 0.9760\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0764 - acc: 0.9781\n",
      "10000/10000 - 0s - loss: 0.1072 - acc: 0.9671\n",
      "test_loss: 0.10717004598863424 \n",
      "test_accuracy: 0.9671000242233276\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test)= mnist.load_data()\n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론 Overview\n",
    "___\n",
    "   \n",
    "입력값이 있는 입력층(input layer), 최종 출력값이 있는 출력층(output layer), 그리고 그 사이에 있는 층인 은닉층(hidden layer)이 있습니다.    \n",
    "  \n",
    "  보통 그림으로 인공신경망을 표현할 때에는 노드를 기준으로 레이어를 표시해서 3개의 레이어라고 생각할 수 있지만, 실제로는 총 2개의 레이어를 가졌습니다. 레이어 개수를 셀 때에는 노드와 노드 사이의 연결하는 부분이 몇 개 존재하는지 세면 보다 쉽게 알 수 있습니다.--> 이해안감 ㅡㅡ\n",
    "  \n",
    "  \n",
    "  Fully-Connected Neural Network 는 MLP와 다른 용어. \n",
    "  \n",
    "  \n",
    "### Parameters/Weights\n",
    "___\n",
    "입력층 - 은닉층, 은닉층 - 출력층 사이에는 각각 행렬(Matrix)가 존재  \n",
    "  \n",
    "  예를들어 입력값이 100개, __은닉 노드가 20개__ 라면 사실 이 입력층_은닉층 사이에는 100*20의 형태를 가진 행렬이 존재합니다. 동일하게, MNIST 데이터처럼 10개의 클래스를 맞추는 문제를 풀기 위해 출력층이 10개의 노드를 가진다면 은닉층-출력층 사이에는 20X10의 형태를 가진 행렬이 존재  \n",
    "  \n",
    "  이 행렬들을 Parameter 혹은 Weight라고 부른다.  \n",
    "  (보통 동일한 뜻이지만 parameter는 bias 노드도 포함한다.)\n",
    "  \n",
    "  인접한 레이어 사이에는 아래와 같은 관계가 성립  \n",
    "  y = W * X + b\n",
    "    \n",
    "    MLP 기반 딥러닝 모델을 Numpy로 다시 만들어 보면 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size = 50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "#바이어스 파라미터 b를 생서앟고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X,W1) + b1 # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.90424991, -0.06582491, -0.4749633 , -0.58904961,  1.31726849,\n",
       "        0.8706947 , -0.20706891, -0.00722531, -0.09464077, -0.8503871 ,\n",
       "       -0.54988447, -0.49131439,  0.34979755,  0.29225337,  0.34283354,\n",
       "        0.77840756,  1.55780878,  0.63100142,  0.42710623,  0.79578388,\n",
       "       -0.5688044 ,  2.10271063, -0.06246996,  0.12921172, -1.40927656,\n",
       "        0.47354288,  1.55185669,  0.51318324, -0.26806489, -0.63239754,\n",
       "       -0.46556499,  1.54568701, -0.58518157,  0.81196124, -2.56901332,\n",
       "        0.93961687,  0.12743145, -0.32600434,  0.94859037,  1.9347575 ,\n",
       "       -0.72616086, -0.72589847, -0.20110807,  1.70398102, -1.05195366,\n",
       "       -1.41251112, -1.23644629, -0.30585811, -1.38249328, -1.06845948])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23-3. 신경망 구성 (2) 활성화 함수와 손실 함수\n",
    "\n",
    "### 활성화 함수 (Activation Functions)\n",
    "___\n",
    "MLP의 중요한 구성 요소는 활성화 함수.  \n",
    "활성화 함수는 보통 비선형 함수를 사용한다. \n",
    "비선형 함수를 MLP 안에 포함시키면 모델의 표현력이 좋아진다.( 비선형 함수가 포함되지 않으면 한 층을 가진 MLP와 다른 점이 없음)\n",
    "   \n",
    "   활성화 함수의 종류는 다음과 같다 \n",
    "   1. Sigmoid\n",
    "   2. Tanh\n",
    "   3. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-627862ef942b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     super(Dense, self).__init__(\n\u001b[0;32m--> 984\u001b[0;31m         activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n\u001b[0m\u001b[1;32m    985\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 은닉층의 출력 a1에 sigmoid를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71182208 0.48354971 0.38344217 0.35685295 0.7887269  0.70489023\n",
      " 0.44841695 0.49819368 0.47635745 0.29935166 0.36589121 0.37958398\n",
      " 0.58656848 0.5725477  0.58487866 0.6853368  0.8260387  0.6527165\n",
      " 0.60518245 0.68907189 0.36151275 0.89116636 0.48438759 0.53225806\n",
      " 0.19634819 0.61622196 0.82518173 0.6255524  0.43338222 0.3469671\n",
      " 0.38566649 0.82428993 0.35774119 0.69252728 0.07115949 0.71902226\n",
      " 0.53181482 0.41921315 0.7208316  0.87377507 0.32603776 0.32609543\n",
      " 0.44989175 0.84605396 0.25885012 0.19583829 0.22505517 0.42412605\n",
      " 0.20060887 0.25569616]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수 구현\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0]) # sigmoid의 출력은 모든 엘리먼트가 0에서 1 사이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요즘은 ReLU 함수를 더 많이 사용한다 그 이유는\n",
    "- Gradient Vanishing 현상이 발생\n",
    "- exp 함수 사용시 비용이 크다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tanh   \n",
    "    - tanh 함수는 함수의 중심값을 0으로 올며 sigmoid의 최저화 과정이느려지는 문제를 해결\n",
    "    - gradient vanishing 문제 존재\n",
    "\n",
    "3. ReLU\n",
    "    - sigmoid, tanh 함수와 비교 시 학습 빠름.\n",
    "    - 연산 비용이 크지 않고, 구현이 매우 간단하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go~\n"
     ]
    }
   ],
   "source": [
    "def affine_layer_forward(X, W, b):\n",
    "    y= np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache\n",
    "\n",
    "print('go~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2506976   0.15168819 -0.18062349 -0.29890787  0.01935595  0.5536517\n",
      " -0.34700708  0.17918931 -0.47877658  0.50332013]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2) # z1이 다시 두번째 레이어의 입력\n",
    "\n",
    "print(a2[0]) # 최종 출력이 output_size만큼의 백터가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis =0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11722637, 0.10617593, 0.07615615, 0.06766042, 0.09301541,\n",
       "       0.1587072 , 0.06448304, 0.10913641, 0.05652216, 0.15091691])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0] # 10개의 숫자 중 하나일 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수 (Loss Function)\n",
    "___\n",
    "__평균제곱오차(MSE: Mean Square Error)__\n",
    "\n",
    "\n",
    "__교차 엔트로피(cross Entropy)__\n",
    "- Cross Entropy는 두 확률분포 사이의 유사도가 클수록 작아지는 값\n",
    "- softmax 값 y_hat은 10개의 숫자 각각의 확률이 대부분 0,1 근처를 오가는 정도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩 하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11722637 0.10617593 0.07615615 0.06766042 0.09301541 0.1587072\n",
      " 0.06448304 0.10913641 0.05652216 0.15091691]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.066680617029902"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis = 1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23-4 경사하강법\n",
    "\n",
    "경사하강법: 각 단계에서의 기울기를 구해 해당 기울기가 가리치는 방향으로 이동하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate를된다면, 잘 정하는게 중요하다\n",
    "\n",
    "1) rate를 매우 크게 했을 경우\n",
    "경사를 내려가는 Step이 매우 크게 된다면, 기울기의 최소값이 아니라 바로 다시 올라갈 수 있다. overshoothg이라고 한다.\n",
    "\n",
    "2) 반대로 rate를 매우 작게했을 경우\n",
    "굉장한 시간이 소요되며, 시간 제한이 있을 경우 경사면이 다 돌기전에 멈춘다.\n",
    "따라서 미리 cost함수를 실행하며 rate의 비율을 찍어보고 테스트 후에 돌리는 것이 좋다!\n",
    "  \n",
    "    \n",
    "Overfitting  \n",
    "  \n",
    "  \n",
    "머신러닝은 학습을 통해서 모델을 만들다보니 학습 데이터에 딱 맞는 모델을 만들 수 있다.\n",
    "이 경우에는 실제 테스트 데이터를 적용하면 적용이 잘 되지 않는다.\n",
    "\n",
    "*Overgitting을 줄이는 방법\n",
    "  \n",
    "  1) 트레이닝 데이터가 많으면 좋다\n",
    "  2) feature 개수를 줄인다.\n",
    "  3) 정규화(일반화-regulaztion)시킨다.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02344527,  0.02123519,  0.01523123,  0.01353208,  0.01860308,\n",
       "        -0.16825856,  0.01289661,  0.02182728,  0.01130443,  0.03018338],\n",
       "       [-0.17675774,  0.02162488,  0.01448357,  0.01450077,  0.01478216,\n",
       "         0.03079713,  0.01461303,  0.02315593,  0.01027108,  0.03252919],\n",
       "       [ 0.02149603,  0.0240629 ,  0.01438086,  0.01400344, -0.1837814 ,\n",
       "         0.03695156,  0.01258247,  0.01957587,  0.00951324,  0.03121505],\n",
       "       [ 0.0237693 , -0.17680012,  0.01350527,  0.01368356,  0.01553494,\n",
       "         0.03505766,  0.0121488 ,  0.01914191,  0.0105897 ,  0.03336899],\n",
       "       [ 0.02254   ,  0.02234497,  0.01502386,  0.01239339,  0.01737142,\n",
       "         0.02845726,  0.01488629,  0.01738709,  0.01209791, -0.16250219]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy # softmax값의 출력으로 loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23-5 오차역전파법이란?\n",
    "\n",
    "기울기들을 입력층까지 전달하며 파라미터들을 조정해 나갈 수 있을까요? 이 과정에서 쓰이는 개념이 오차역전파법(Backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy,W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis = 0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10645674 0.07661421 0.11119531 0.06818998 0.14758097 0.12919525\n",
      "  0.12442905 0.08984575 0.04911006 0.09738268]\n",
      " [0.1201585  0.0681808  0.08975143 0.08430668 0.15169484 0.12166741\n",
      "  0.13424342 0.10220282 0.05133659 0.07645751]\n",
      " [0.12278131 0.0811323  0.11469548 0.07757755 0.14801436 0.11842983\n",
      "  0.10078021 0.11047744 0.05146025 0.07465126]\n",
      " [0.11045187 0.06900392 0.104791   0.071402   0.15180773 0.10767011\n",
      "  0.12943798 0.11030525 0.05768182 0.08744831]\n",
      " [0.13161297 0.07340368 0.09575113 0.08027874 0.1337489  0.1141776\n",
      "  0.13287518 0.11005451 0.05129072 0.07680657]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss: 2.2631754304800227\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_ont_hot_label(Y_digit, 10) # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss:', Loss)\n",
    "\n",
    "dy = (y_hat - t)/ X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1)*dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트\n",
    "learning_rate=0.1\n",
    "W1, b1, W2, b2 = update_params(W1,b1, W2,b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23-6 모델 학습 Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std*np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std*np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_ont_hot_label(Y,10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "    \n",
    "    if verbose:\n",
    "        print('-------------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss',Loss)\n",
    "        \n",
    "    dy = (y_hat-t) / X.shape[0]\n",
    "    dz1, dW2, db2, = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1)*dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "[[0.1171     0.15830946 0.07874161 0.10007629 0.07719725 0.1290722\n",
      "  0.12584864 0.07895927 0.07201578 0.0626795 ]\n",
      " [0.11714037 0.14452867 0.08066462 0.09669471 0.08533504 0.11712469\n",
      "  0.11672343 0.07625519 0.08956766 0.07596561]\n",
      " [0.14516133 0.1284979  0.07619369 0.09091259 0.07982995 0.11147801\n",
      "  0.14867563 0.08093971 0.0721466  0.0661646 ]\n",
      " [0.1067322  0.13924183 0.0872282  0.09712625 0.07810169 0.12868761\n",
      "  0.13694858 0.07026543 0.08600053 0.06966767]\n",
      " [0.11979432 0.13403693 0.08273048 0.09508308 0.06443005 0.11872501\n",
      "  0.14125779 0.07325104 0.08144877 0.08924254]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss 2.2215125304835746\n",
      "-------------\n",
      "[[0.13025075 0.16936733 0.06727686 0.08408857 0.09051951 0.15195159\n",
      "  0.10020574 0.06832143 0.06216325 0.07585499]\n",
      " [0.13596728 0.15355614 0.06880545 0.08124676 0.09977769 0.13203945\n",
      "  0.09369308 0.06647959 0.07759544 0.09083912]\n",
      " [0.16017787 0.13776999 0.06631019 0.07793803 0.09986179 0.1244754\n",
      "  0.11984297 0.07144034 0.0629139  0.07926953]\n",
      " [0.11755199 0.156213   0.07584425 0.08258483 0.0922095  0.14467802\n",
      "  0.11041571 0.06169818 0.07457822 0.08422629]\n",
      " [0.13290334 0.14464427 0.07150353 0.08062487 0.07632164 0.13401379\n",
      "  0.11291398 0.06432985 0.07046953 0.11227519]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss 2.045367906117639\n",
      "-------------\n",
      "[[0.13951984 0.17528977 0.05767721 0.07120684 0.10197711 0.17148164\n",
      "  0.08191373 0.05909502 0.0537609  0.08807794]\n",
      " [0.15214424 0.15796415 0.05891302 0.06880071 0.11230866 0.14296659\n",
      "  0.07716218 0.05794323 0.06735156 0.10444567]\n",
      " [0.17040983 0.14290959 0.05782247 0.06726775 0.12003391 0.13343466\n",
      "  0.0990552  0.06297542 0.05491612 0.09117506]\n",
      " [0.12480746 0.16953099 0.06611139 0.07070925 0.10467033 0.15619664\n",
      "  0.09127706 0.05411699 0.06477422 0.09780567]\n",
      " [0.1418145  0.15068623 0.06191745 0.06878461 0.08667087 0.1448439\n",
      "  0.09254047 0.0563631  0.06098072 0.13539814]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss 1.908088310937091\n",
      "-------------\n",
      "[[0.14588837 0.17805643 0.04982771 0.06093226 0.11151328 0.1881581\n",
      "  0.06840285 0.05136565 0.0467984  0.09905695]\n",
      " [0.16625616 0.15951262 0.0508356  0.05885911 0.12289394 0.15069583\n",
      "  0.06487332 0.05074323 0.05882886 0.11650133]\n",
      " [0.17707596 0.14540639 0.05072157 0.05859307 0.13998866 0.13918128\n",
      "  0.0835365  0.05572005 0.04819011 0.10158641]\n",
      " [0.12942339 0.18050431 0.05802148 0.06113406 0.11537157 0.16416498\n",
      "  0.07701396 0.04766703 0.05660849 0.11009072]\n",
      " [0.14757807 0.15379544 0.05395347 0.0592127  0.09536753 0.15207368\n",
      "  0.07739126 0.0495397  0.05303703 0.15805112]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss 1.797545912885348\n",
      "-------------\n",
      "[[0.15021725 0.17903823 0.04344105 0.05271343 0.11917006 0.20265443\n",
      "  0.05814287 0.04496267 0.04107446 0.10858556]\n",
      " [0.17891682 0.15942273 0.04426859 0.05089432 0.13159578 0.15605467\n",
      "  0.05548515 0.04474305 0.05179627 0.12682261]\n",
      " [0.18122636 0.14631682 0.04481936 0.05152628 0.15945827 0.1425454\n",
      "  0.07162431 0.04958452 0.04258076 0.11031792]\n",
      " [0.13219052 0.19015505 0.05134792 0.05340338 0.12430477 0.16951809\n",
      "  0.06609982 0.04225439 0.04986496 0.1208611 ]\n",
      " [0.15112642 0.15515399 0.04738606 0.0514687  0.10241624 0.15661689\n",
      "  0.06582128 0.0437862  0.04645329 0.17977092]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss 1.7058095510164262\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립ㄴ디ㅏ.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2,_ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23-7 추론 과정 구현과 정확도(Accuracy) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, x):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1531612 , 0.17908182, 0.03822781, 0.04608571, 0.12506892,\n",
       "       0.2156145 , 0.05016994, 0.03966276, 0.03636257, 0.11656478])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100]에 대해 몯레 추론을 시도합니다.\n",
    "\n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x,y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "    # t = np.argmax(t,axis=1)\n",
    "    \n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1531612  0.17908182 0.03822781 0.04608571 0.12506892 0.2156145\n",
      " 0.05016994 0.03966276 0.03636257 0.11656478]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.08\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_ont_hot_label(Y,10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d8f8a4a2a9fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23-8 전체 학습 사이클 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size, hidden_size, output_size, weight_size_init_std=0.01):\n",
    "    \n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
